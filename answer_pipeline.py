from pipeline.iterative import IterativePipeline


actor_models = [
    "gpt-4.1-mini", 
    "gpt-4.1-mini", 
    "gpt-4.1-nano", 
    "gemini-2.0-flash", 
    'nim:meta/llama-4-maverick-17b-128e-instruct', 
    'nim:meta/llama-3.2-11b-vision-instruct',
    'nim:meta/llama-3.2-90b-vision-instruct',
    'nim:meta/llama-4-maverick-17b-128e-instruct',
    'google:gemma-3-27b-it',
    'google:gemma-3-12b-it'
]

critic_models = [
    # 'gemini-2.5-flash-preview-05-20',
    # 'gemini-2.0-flash',
    'gpt-4.1'
]

evaluate_models = [
    'gpt-4.1'
]

environment = [
    'html',
    'python'
]


import sys
import os
import json
import random
from datetime import datetime
import uuid
current_dir = os.path.dirname(os.path.abspath(__file__))

# Import parameters from answer_pipeline
from answer_pipeline import actor_models, critic_models, environment

# Import necessary pipeline components
from pipeline.iterative import IterativePipeline
from pipeline.module import Module, ModuleConfig
from agent import ActorConfig, CriticConfig, VisionCriticConfig, TextCriticConfig, VisionCritic
from pipeline.execution import HtmlEnv, HtmlEnvConfig, PythonEnv, PythonEnvConfig
from utils import open_image, merge_images

def load_questions(jsonl_file='chart_modification.jsonl', done_file=None):
    """Load questions from the JSONL file generated by generate_questions.py"""
    questions = []
    done_images = set()
    if isinstance(done_file, str) and os.path.exists(done_file):
        with open(done_file, 'r') as f:
            for line in f:
                data = json.loads(line)
                done_images.add(data['image_path'])

    print(f"Found {len(done_images)} done images in {done_file}")

    if os.path.exists(jsonl_file):
        with open(jsonl_file, 'r') as f:
            for line in f:
                data = json.loads(line)
                # Skip if image already processed
                if data['image_path'] in done_images:
                    continue
                questions.append(data)
    return questions

def setup_pipeline(actor_model, critic_model, env_type, logger='mongodb'):
    """Setup pipeline with randomly selected parameters"""
    # Set up environment
    if env_type == 'html':
        env = HtmlEnv(config=HtmlEnvConfig(name="HTML Environment"))
    elif env_type == 'python':
        env = PythonEnv(config=PythonEnvConfig(name="Python Environment"))
    else:
        raise ValueError(f"Unsupported environment: {env_type}")
    
    force_image_path = logger is not None
    # Set up module configurations
    actor_config = ActorConfig(name="Chart Actor", model_name=actor_model, code=env_type, image_path=force_image_path, logger=logger, rotate='gpt' not in actor_model)
    vision_critic_config = VisionCriticConfig(name="Vision Critic", model_name=critic_model, image_path=force_image_path, logger=logger, rotate='gpt' not in critic_model)
    text_critic_config = TextCriticConfig(name="Text Critic", model_name=critic_model, code=env_type, image_path=force_image_path, logger=logger, rotate='gpt' not in critic_model)
    # Create critic configuration
    critic_config = CriticConfig(
        name="Chart Critic", 
        vision=vision_critic_config, 
        text=text_critic_config, 
        model_name=critic_model,
        image_path=force_image_path
    )
    
    # Create module and pipeline
    module_config = ModuleConfig(name="Chart Module", actor_config=actor_config, critic_config=critic_config, image_path=force_image_path)
    module = Module(config=module_config)
    
    # Generate unique run name
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_id = str(uuid.uuid4())[:8]
    run_name = f"chart_run_{timestamp}_{run_id}"
    
    pipeline = IterativePipeline(module=module, env=env, run_name=run_name, debug=True)
    
    return pipeline

def setup_evaluation(evaluate_model, logger=None):
    """Setup evaluation module with specified model"""
    evaluate_config = VisionCriticConfig(
        name="Evaluation Critic", 
        model_name=evaluate_model, 
        image_path=True, 
        logger=logger
    )
    return VisionCritic(config=evaluate_config)

def save_results(question_data, actor, critic, env_type, results, output_dir="results"):
    """Save the results of processing a question"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    image_id = os.path.basename(question_data["image_path"]).split('.')[0]
    
    result_data = {
        "question": question_data["question"],
        "image_path": question_data["image_path"],
        "actor_model": actor,
        "critic_model": critic,
        "environment": env_type,
        "results": results,
        "timestamp": timestamp
    }
    
    output_file = f"{output_dir}/result_chart.jsonl"
    with open(output_file, 'a') as f:
        f.write(json.dumps(result_data) + '\n')

    return output_file

def save_evaluation_results(question_data, actor, critic, env_type, **kwargs):
    """Save the results of processing a question"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    result_data = {
        'image_path': question_data["image_path"],
        "question": question_data["question"],
        "actor_model": actor,
        "critic_model": critic,
        "environment": env_type,
        "timestamp": timestamp,
        **kwargs
    }

    output_file = f"results/{actor.replace('/', '_')}_{critic.replace('/', '_')}_{env_type}_eval.jsonl"
    with open(output_file, 'a') as f:
        f.write(json.dumps(result_data) + '\n')

    return output_file



import concurrent.futures
import threading


def solve_question(question_data, actor_model, critic_model, env_type):
    # Setup pipeline
    pipeline = setup_pipeline(actor_model, critic_model, env_type, logger=None)

    # Process the question
    image_path = question_data["image_path"]
    task = question_data["question"]

    if not os.path.exists(image_path):
        print(f"Image not found: {image_path}, skipping")
        return None

    # Run the pipeline
    results = pipeline.act(request=task, image=image_path)

    return results

def process_question(question_data, idx, total):
    try:
        # Randomly select parameters
        actor = random.choice(actor_models)
        critic = random.choice(critic_models)
        env_type = random.choice(environment)
        
        print_lock = threading.Lock()
        with print_lock:
            print(f"\n[{idx+1}/{total}] Processing question with:")
            print(f"  - Actor: {actor}")
            print(f"  - Critic: {critic}")
            print(f"  - Environment: {env_type}")
            print(f"  - Question: {question_data['question']}")
        
        result = solve_question(question_data, actor, critic, env_type)
        
        # Save results
        output_file = save_evaluation_results(question_data, actor, critic, env_type, result)
        with print_lock:
            print(f"  - Results saved to: {output_file}")
        
        return output_file
    except Exception as e:
        with print_lock:
            print(f"Error processing question {idx+1}: {str(e)}")
        return None
    


def evaluate_question(question_data, actor_model, critic_model, env_type, idx, total):
    try:
        print_lock = threading.Lock()
        with print_lock:
            print(f"\n[{idx+1}/{total}] Evaluating question with:")
            print(f"  - Actor: {actor_model}")
            print(f"  - Critic: {critic_model}")
            print(f"  - Question: {question_data['question']}")
            print(f"  - Environment: {env_type}")
            print(f"  - Question: {question_data['question']}")
        
        # Setup pipeline

        evaluator = setup_evaluation('gpt-4.1')
        

        results = solve_question(question_data, actor_model, critic_model, env_type)

        save_params = dict()

        if not results:
            save_params['score'] = 0
            save_params['number_iter'] = 0
            save_params['first_score'] = 0

        else:
            last_image = results[-1].get('output_image', None)

            combined_image = merge_images([open_image(question_data["image_path"]), open_image(last_image)],
                                           titles=[f"Original Image", f"Generated Image"])
            
            score = evaluator.act(request=question_data["question"], action_image=combined_image).get('score', 0)
            number_iter = len(results)

            save_params['score'] = score
            save_params['number_iter'] = number_iter

            if len(results) == 1:
                # If only one iteration, use the first score
                save_params['first_score'] = score
            else:
                first_image = results[0].get('output_image', None)
                combined_image = merge_images([open_image(question_data["image_path"]), open_image(first_image)],                                               titles=[f"Original Image", f"First Generated Image"])
                save_params['first_score'] = evaluator.act(request=question_data["question"], action_image=combined_image).get('score', 0)


        # Save results
        output_file = save_evaluation_results(question_data, actor_model, critic_model, env_type, **save_params)
        with print_lock:
            print(f"  - Results saved to: {output_file}")

        return output_file
    except Exception as e:
        with print_lock:
            print(f"Error processing question {idx+1}: {str(e)}")
        return None
    


def main_threaded(max_workers=2):
    """Multithreaded version of main function to process questions concurrently"""
    # Load all questions
    questions = load_questions(done_file='results/result_chart.jsonl')[::-1]
    print(f"Loaded {len(questions)} questions")
    print(f"Using {max_workers} threads")
    
    # Use ThreadPoolExecutor for concurrent execution
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all tasks
        futures = {executor.submit(process_question, q_data, i, len(questions)): i 
                  for i, q_data in enumerate(questions)}
        
        # Process results as they complete
        completed = 0
        for future in concurrent.futures.as_completed(futures):
            completed += 1
            result = future.result()
            print(f"Progress: {completed}/{len(questions)}", end="\r")
    
    print("\nAll questions processed!")


def eval_threaded(max_workers=2, dataset='chart_modification_eval.jsonl', actor_model='gpt-4.1-mini', critic_model='gpt-4.1-mini', env_type='html'):
    """Multithreaded version of main function to process questions concurrently"""
    # Load all questions
    evaluation_path = os.path.join('test', dataset)
    questions = load_questions(jsonl_file=evaluation_path, done_file=f"results/{actor_model}_{critic_model}_{env_type}_eval.jsonl")[::-1]
    print(f"Loaded {len(questions)} questions")
    print(f"Using {max_workers} threads")
    
    # Use ThreadPoolExecutor for concurrent execution
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all tasks
        futures = {executor.submit(evaluate_question,  q_data, actor_model, critic_model, env_type, i, len(questions)): i
                  for i, q_data in enumerate(questions)}
        
        # Process results as they complete
        completed = 0
        for future in concurrent.futures.as_completed(futures):
            completed += 1
            result = future.result()
            print(f"Progress: {completed}/{len(questions)}", end="\r")

    # for i, q_data in enumerate(questions):
    #     result = evaluate_question(q_data, actor_model, critic_model, env_type, i, len(questions))
    #     if result:
    #         print(f"Processed question {i+1}/{len(questions)}: {result}")
    #     else:
    #         print(f"Failed to process question {i+1}/{len(questions)}")
        
    
    print("\nAll questions processed!")

if __name__ == "__main__":
    # Use main_threaded() instead of main()
    model = 'google:gemma-3-27b-it'
    environment = 'python'
    eval_threaded(actor_model=model, critic_model=model, env_type=environment)